{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def NumDots(url):\n",
    "    \"\"\"\n",
    "    Counts the number of . in a url. \n",
    "    Returns the number of dots present in the URL.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    num_dots = parsed_url.netloc.count('.')\n",
    "    if parsed_url.path:\n",
    "        num_dots += parsed_url.path.count('.')\n",
    "    if url.endswith('.'):\n",
    "        num_dots -= 1\n",
    "    return num_dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def SubdomainLevel(url):\n",
    "    \"\"\"\n",
    "    Computes the number of subdomain levels in the URL.\n",
    "    \"\"\"\n",
    "\n",
    "    hostname = urlparse(url).hostname\n",
    "    subdomains = hostname.split('.')\n",
    "    num_subdomains = len(subdomains) - 1\n",
    "    return num_subdomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def PathLevel(url):\n",
    "    \"\"\"\n",
    "    Computes the number of path levels in the URL.\n",
    "    \"\"\"\n",
    "    \n",
    "    path = urlparse(url).path\n",
    "    num_levels = path.count('/') - 1\n",
    "    return num_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def UrlLength(url):\n",
    "    \"\"\"\n",
    "    Computes the length of the URL.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    return len(parsed_url.geturl())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumDash(url):\n",
    "    \"\"\"\n",
    "    Counts the number of - in a url. \n",
    "    Returns the number of dashes present in the URL.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    num_dashes = parsed_url.netloc.count('-')\n",
    "    if parsed_url.path:\n",
    "        num_dashes += parsed_url.path.count('-')\n",
    "    if url.endswith('-'):\n",
    "        num_dashes -= 1\n",
    "    return num_dashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumDashInHostName(url):\n",
    "    \"\"\"\n",
    "    Counts the number of dashes in the hostname component of a URL.\n",
    "    \"\"\"\n",
    "    hostname = urlparse(url).hostname\n",
    "    num_dashes_in_host = hostname.count('-')\n",
    "    return num_dashes_in_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def AtSymbol(url):\n",
    "    \"\"\"\n",
    "    Checks if @ symbol is present in the URL.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    if '@' in parsed_url.netloc:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def TildeSymbol(url):\n",
    "    \"\"\"\n",
    "    Checks if ~ symbol is present in the URL.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    if '~' in parsed_url.path:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumUnderscore(url):\n",
    "    \"\"\"\n",
    "    Counts the number of _ in the URL.\n",
    "    Returns the number of underscores present in the parsed URL.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    num_underscores = parsed_url.netloc.count('_')\n",
    "    if parsed_url.path:\n",
    "        num_underscores += parsed_url.path.count('_')\n",
    "    if url.endswith('_'):\n",
    "        num_underscores -= 1\n",
    "    return num_underscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def NumPercent(url):\n",
    "    \"\"\"\n",
    "    Counts the number of % in the URL.\n",
    "    Returns the number of percentage symbols present in the URL.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    num_percent = parsed_url.path.count('%') + parsed_url.query.count('%')\n",
    "    return num_percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def NumQueryComponents(url):\n",
    "    \"\"\"\n",
    "    Counts the number of query components in the URL.\n",
    "    \"\"\"\n",
    "    query = urlparse(url).query\n",
    "    components = query.split('&')\n",
    "    num_components = len(components)\n",
    "    return num_components\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def NumAmpersand(url):\n",
    "    \"\"\"\n",
    "    Counts the number of ampersands in the URL.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    num_ampersands = parsed_url.query.count('&')\n",
    "    return num_ampersands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def NumHash(url):\n",
    "    \"\"\"\n",
    "    Counts the number of # in the URL.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    num_hashes = parsed_url.fragment.count('#')\n",
    "    return num_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def NumNumericChars(url):\n",
    "    \"\"\"\n",
    "    Counts the number of numeric characters in the URL.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    num_numeric = sum(c.isdigit() for c in parsed_url.path + parsed_url.query)\n",
    "    return num_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def NoHttps(url):\n",
    "    \"\"\"\n",
    "    Checks if https is present in the URL.\n",
    "    If https is present then it returns 0,\n",
    "    and if https is not present then it returns 1.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    if parsed_url.scheme == 'https':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def RandomString(url):\n",
    "    \"\"\"\n",
    "    Checks if random string is present in the URL.\n",
    "    If random string is present then it returns 1,\n",
    "    and if random string is not present then it returns 0.\n",
    "    \"\"\"\n",
    "    pattern = r\"\\d{4,}\" # match any sequence of 4 or more digits\n",
    "    if re.search(pattern, url):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def IpAddress(url):\n",
    "    \"\"\"\n",
    "    Checks if IP address is present in the URL.\n",
    "    If IP address is present then it is phishing hence the function returns 1,\n",
    "    and if IP address is not present then it is benign hence return 0.\n",
    "    \"\"\"\n",
    "    if re.search(r\"^(http|https|ftp)://\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", url) or \\\n",
    "       re.search(r\"^(http|https|ftp)://\\w+\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", url) or \\\n",
    "       re.search(r\"^(http|https|ftp)://\\w+\\.\\w+\\.\\d{1,3}\\.\\d{1,3}\", url) or \\\n",
    "       re.search(r\"^(http|https|ftp)://\\w+\\.\\w+\\.\\w+\\.\\d{1,3}\", url) or \\\n",
    "       re.search(r\"\\?\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", url):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DomainInSubdomains(url):\n",
    "    \"\"\"\n",
    "    Checks if domain is present in the subdomains of the URL.\n",
    "    If domain is present then it is phishing hence the function returns 1,\n",
    "    and if domain is not present then it is benign hence return 0.\n",
    "    \"\"\"\n",
    "    hostname = urlparse(url).hostname\n",
    "    subdomains = hostname.split('.')\n",
    "    domain = '.'.join(subdomains[-2:])\n",
    "    if domain in subdomains[:-2]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DomainInPaths(url):\n",
    "    \"\"\"\n",
    "    Checks if domain is present in the paths of the URL.\n",
    "    If domain is present then it is phishing hence the function returns 1,\n",
    "    and if domain is not present then it is benign hence return 0.\n",
    "    \"\"\"\n",
    "    hostname = urlparse(url).netloc\n",
    "    path = urlparse(url).path\n",
    "    query = urlparse(url).query\n",
    "    fragment = urlparse(url).fragment\n",
    "    num_occurrences = path.count(hostname) + query.count(hostname) + fragment.count(hostname)\n",
    "    if num_occurrences >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HttpsInHostname(url):\n",
    "    \"\"\"\n",
    "    Checks if https is present in the hostname of the URL.\n",
    "    If https is present then it is phishing hence the function returns 1,\n",
    "    and if https is not present then it is benign hence return 0.\n",
    "    \"\"\"\n",
    "    hostname = urlparse(url).netloc\n",
    "    if re.search(r\"(h|x{2})ttps?\", hostname):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlsplit\n",
    "\n",
    "def get_hostname_length(url):\n",
    "    \"\"\"\n",
    "    Returns the length of the hostname component of the URL.\n",
    "    \n",
    "    Args:\n",
    "    url (str): The URL to extract the hostname from.\n",
    "    \n",
    "    Returns:\n",
    "    int: The length of the hostname component of the URL.\n",
    "    \"\"\"\n",
    "    hostname = urlsplit(url).hostname\n",
    "    return len(hostname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def PathLength(url):\n",
    "    \"\"\"\n",
    "    Returns the length of the path component of the URL.\n",
    "    \n",
    "    Args:\n",
    "    url (str): The URL to extract the path from.\n",
    "    \n",
    "    Returns:\n",
    "    int: The length of the path component of the URL.\n",
    "    \"\"\"\n",
    "    path = urlparse(url).path\n",
    "    return len(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QueryLength(url):\n",
    "    \"\"\"\n",
    "    Returns the length of the query string component of the URL.\n",
    "    \n",
    "    Args:\n",
    "    url (str): The URL to extract the query string from.\n",
    "    \n",
    "    Returns:\n",
    "    int: The length of the query string component of the URL.\n",
    "    \"\"\"\n",
    "    query_length = urlparse(url).query\n",
    "    return len(query_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoubleSlashInPath(url):\n",
    "    \"\"\"\n",
    "    Checks if double slash is present in the path of the URL.\n",
    "    If double slash is present then it returns True,\n",
    "    and if double slash is not present then it returns False.\n",
    "    \"\"\"\n",
    "    path = urlparse(url).path\n",
    "    if path.count('//') >= 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from tldextract import extract\n",
    "\n",
    "def NumSensitiveWords(url):\n",
    "    \"\"\"\n",
    "    Counts the number of sensitive words in the URL and the website content.\n",
    "    If the number of sensitive words is less than or equal to 1 then it is benign hence return 0,\n",
    "    and if the number of sensitive words is more than 1 then it is phishing hence the function returns 1.\n",
    "    \"\"\"\n",
    "    sensitive_words = ['confirm', 'account', 'banking', 'secure', 'login', 'ebayisapi', 'webscr', 'signin', 'submit', 'password', 'authenticate', 'lucky', 'bonus', 'ssl', 'banking', 'bank', 'secure', 'update', 'money', 'ebay']\n",
    "    num_sensitive = 0\n",
    "    try:\n",
    "        # Extract the domain name from the URL\n",
    "        domain = extract(url).domain\n",
    "        \n",
    "        # Check if the domain name is in the URL\n",
    "        if domain in url:\n",
    "            num_sensitive += 1\n",
    "        \n",
    "        # Check if the sensitive words are in the URL\n",
    "        for word in sensitive_words:\n",
    "            if word in url:\n",
    "                num_sensitive += 1\n",
    "        \n",
    "        # Use a head request to fetch the website content\n",
    "        response = requests.head(url)\n",
    "        content_type = response.headers.get('content-type')\n",
    "        \n",
    "        # Check if the content type is HTML\n",
    "        if 'text/html' in content_type:\n",
    "            # Use a get request to fetch the website content\n",
    "            response = requests.get(url)\n",
    "            content = response.text\n",
    "            \n",
    "            # Count the number of sensitive words in the website content\n",
    "            num_sensitive += sum(content.lower().count(word) for word in sensitive_words)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if num_sensitive <= 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from tldextract import extract\n",
    "import spacy\n",
    "\n",
    "def EmbeddedBrandName(url):\n",
    "    \"\"\"\n",
    "    Checks if brand name is present in the URL but is not the same as the domain name.\n",
    "    If brand name is present and is not the same as the domain name, then it is phishing hence the function returns 1,\n",
    "    and if brand name is not present or is the same as the domain name, then it is benign hence return 0.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract the domain name from the URL\n",
    "        domain = extract(url).domain\n",
    "        \n",
    "        # Use a head request to fetch the website content\n",
    "        response = requests.head(url)\n",
    "        content_type = response.headers.get('content-type')\n",
    "        \n",
    "        # Check if the content type is HTML\n",
    "        if 'text/html' in content_type:\n",
    "            # Use a get request to fetch the website content\n",
    "            response = requests.get(url)\n",
    "            content = response.text\n",
    "            \n",
    "            # Use spaCy for natural language processing\n",
    "            nlp = spacy.load('en_core_web_sm')\n",
    "            doc = nlp(content)\n",
    "            \n",
    "            # Extract the named entities from the website content\n",
    "            entities = [entity.text.lower() for entity in doc.ents if entity.label_ == 'ORG']\n",
    "            \n",
    "            # Check if the brand name is present in the URL but is not the same as the domain name\n",
    "            for entity in entities:\n",
    "                if entity in url.lower() and entity != domain:\n",
    "                    return 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_pct_ext_hyperlinks(url):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all hyperlinks in the page\n",
    "        all_links = soup.find_all('a')\n",
    "\n",
    "        # Count the number of external hyperlinks\n",
    "        external_links = 0\n",
    "\n",
    "        for link in all_links:\n",
    "            href = link.get('href', '')\n",
    "            if href.startswith('http://') or href.startswith('https://'):\n",
    "                external_links += 1\n",
    "\n",
    "        # Calculate the percentage of external hyperlinks\n",
    "        total_links = len(all_links)\n",
    "        if total_links > 0:\n",
    "            pct_ext_links = external_links / total_links\n",
    "        else:\n",
    "            pct_ext_links = 0\n",
    "\n",
    "        return pct_ext_links\n",
    "    else:\n",
    "        # If the request was not successful, return an 0\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def PctExtResourceUrls(url):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of external resource URLs on a given web page.\n",
    "    \n",
    "    Args:\n",
    "    url (str): The URL of the web page to analyze.\n",
    "    \n",
    "    Returns:\n",
    "    float: The percentage of external resource URLs on the page, as a floating-point number between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all resource URLs in the page\n",
    "        all_urls = soup.find_all(['img', 'script', 'link'])\n",
    "\n",
    "        # Count the number of external resource URLs\n",
    "        external_urls = 0\n",
    "\n",
    "        for url in all_urls:\n",
    "            href = url.get('href', '')\n",
    "            src = url.get('src', '')\n",
    "            if href.startswith('http://') or href.startswith('https://') or src.startswith('http://') or src.startswith('https://'):\n",
    "                parsed_url = urlparse(href if href else src)\n",
    "                if parsed_url.netloc:\n",
    "                    external_urls += 1\n",
    "\n",
    "        # Calculate the percentage of external resource URLs\n",
    "        total_urls = len(all_urls)\n",
    "        if total_urls > 0:\n",
    "            pct_ext_urls = external_urls / total_urls\n",
    "        else:\n",
    "            pct_ext_urls = 0\n",
    "\n",
    "        return pct_ext_urls\n",
    "    else:\n",
    "        # If the request was not successful, return an error value\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def ExtFavicon(url):\n",
    "    try:\n",
    "        # Send an HTTP HEAD request to the URL to minimize data transfer\n",
    "        response = requests.head(url)\n",
    "\n",
    "        # Check if the request was successful (status code 200 or 301/302 for redirects)\n",
    "        if response.status_code in (200, 301, 302):\n",
    "            # Check for the 'Content-Type' header to ensure it's an HTML page\n",
    "            content_type = response.headers.get('Content-Type', '').lower()\n",
    "            if content_type.startswith('text/html'):\n",
    "                # Send a GET request to fetch the full page content\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()  # Raise an exception for any HTTP error\n",
    "\n",
    "                # Parse the HTML content of the page using BeautifulSoup\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # Find the favicon link in the page\n",
    "                favicon_link = soup.find('link', rel='shortcut icon') or soup.find('link', rel='icon')\n",
    "\n",
    "                # Extract the URL of the favicon\n",
    "                if favicon_link:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                # The response is not HTML, so no favicon can be present\n",
    "                return 0\n",
    "        else:\n",
    "            # If the request was not successful, return 0\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def InsecureForms(url):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all form elements in the page\n",
    "            forms = soup.find_all('form')\n",
    "\n",
    "            # Check if any of the forms use HTTP instead of HTTPS in the action attribute\n",
    "            for form in forms:\n",
    "                action = form.get('action', '')\n",
    "                if action:\n",
    "                    parsed_url = urlparse(action)\n",
    "                    if parsed_url.scheme and parsed_url.scheme.lower() == 'http':\n",
    "                        return 1\n",
    "\n",
    "            # If no insecure forms were found, return 0\n",
    "            return 0\n",
    "        else:\n",
    "            # If the request was not successful, return an error value\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during the process\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def RelativeFormAction(url):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all form elements in the page\n",
    "            forms = soup.find_all('form')\n",
    "\n",
    "            # Check if any of the forms have a relative action URL\n",
    "            for form in forms:\n",
    "                action = form.get('action', '')\n",
    "                if action and not action.startswith(('http://', 'https://')):\n",
    "                    return 1  # At least one relative form action exists\n",
    "\n",
    "            # If no relative form actions were found, return 0\n",
    "            return 0\n",
    "        else:\n",
    "            # If the request was not successful, return an error value\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during the process\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def ExtFormAction(url):\n",
    "    \"\"\"\n",
    "    Checks if external form action is present in the URL.\n",
    "    If external form action is present then it is phishing hence the function returns 1,\n",
    "    and if external form action is not present then it is benign hence return 0.\n",
    "    \n",
    "    Args:\n",
    "    url (str): The URL of the web page to check.\n",
    "    \n",
    "    Returns:\n",
    "    int: 1 if the web page is a phishing attempt, 0 otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all form elements in the page\n",
    "            forms = soup.find_all('form')\n",
    "\n",
    "            # Check if any of the forms have an external action URL\n",
    "            for form in forms:\n",
    "                action = form.get('action', '')\n",
    "                if action and action.startswith(('http://', 'https://')):\n",
    "                    return 1  # External form action exists\n",
    "\n",
    "            # If no external form actions were found, return 0\n",
    "            return 0\n",
    "        else:\n",
    "            # If the request was not successful, return an error value\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during the process\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def AbnormalFormAction(url):\n",
    "    \"\"\"\n",
    "    Checks if abnormal form action is present in the URL.\n",
    "    If abnormal form action is present then it is phishing hence the function returns 1,\n",
    "    and if abnormal form action is not present then it is benign hence return 0.\n",
    "    \n",
    "    Args:\n",
    "    url (str): The URL of the web page to check.\n",
    "    \n",
    "    Returns:\n",
    "    int: 1 if the web page is a phishing attempt, 0 otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all form elements in the page\n",
    "            forms = soup.find_all('form')\n",
    "\n",
    "            # Check if any of the forms have an abnormal action URL\n",
    "            for form in forms:\n",
    "                action = form.get('action', '')\n",
    "                if action and not action.startswith(('http://', 'https://', '/')):\n",
    "                    return 1  # Abnormal form action exists\n",
    "\n",
    "            # If no abnormal form actions were found, return 0\n",
    "            return 0\n",
    "        else:\n",
    "            # If the request was not successful, return an error value\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during the process\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def PctNullSelfRedirectHyperlinks(url):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all hyperlinks (a elements) in the page\n",
    "            hyperlinks = soup.find_all('a')\n",
    "\n",
    "            # Check if there are any self-redirect hyperlinks\n",
    "            self_redirect_count = 0\n",
    "            for link in hyperlinks:\n",
    "                href = link.get('href', '')\n",
    "\n",
    "                # Check if the href attribute indicates a self-redirect\n",
    "                if href and href == url:\n",
    "                    self_redirect_count += 1\n",
    "\n",
    "            # Calculate the percentage of no self-redirect hyperlinks\n",
    "            no_self_redirect_count = len(hyperlinks) - self_redirect_count\n",
    "            pct_no_self_redirect = no_self_redirect_count / len(hyperlinks)\n",
    "\n",
    "            return pct_no_self_redirect\n",
    "\n",
    "        else:\n",
    "            # If the request was not successful, return an error value\n",
    "            return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during the process\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def FrequentDomainNameMismatch(url):\n",
    "    try:\n",
    "        # Parse the URL to extract the domain\n",
    "        parsed_url = urlparse(url)\n",
    "        url_domain = parsed_url.netloc\n",
    "\n",
    "        # Split the domain into subdomains\n",
    "        subdomains = url_domain.split('.')\n",
    "\n",
    "        # Check for frequent subdomain mismatches\n",
    "        frequent_mismatches = 0\n",
    "        for i in range(len(subdomains) - 1):\n",
    "            subdomain = '.'.join(subdomains[i:])\n",
    "            if url_domain != subdomain:\n",
    "                frequent_mismatches += 1\n",
    "\n",
    "        # Determine if frequent mismatches exist\n",
    "        if frequent_mismatches > 0:\n",
    "            return 1  # Frequent domain name mismatches\n",
    "        else:\n",
    "            return 0  # No frequent mismatches\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during the process\n",
    "        return 0  # Return 0 on error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def DetectFakeLinkInStatusBar(url):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            return 0  # Unable to access the URL\n",
    "\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract all links (a elements) from the page\n",
    "        links = soup.find_all('a')\n",
    "\n",
    "        # Check each link for potential fake attributes or behaviors\n",
    "        for link in links:\n",
    "            href = link.get('href', '')\n",
    "\n",
    "            # Check if the link opens a new window or tab\n",
    "            if link.get('target') == '_blank':\n",
    "                return 1  # Fake link detected\n",
    "\n",
    "            # Check for JavaScript-based actions (e.g., onclick)\n",
    "            if link.has_attr('onclick'):\n",
    "                return 1  # Fake link detected\n",
    "\n",
    "            # Check for obfuscated URLs in JavaScript\n",
    "            if 'javascript:' in href:\n",
    "                return 1  # Fake link detected\n",
    "\n",
    "            # Check for links that use the \"data:\" scheme\n",
    "            if href.startswith('data:'):\n",
    "                return 1  # Fake link detected\n",
    "\n",
    "            # Check for links with JavaScript in the href\n",
    "            if re.search(r'javascript:', href, re.I):\n",
    "                return 1  # Fake link detected\n",
    "\n",
    "            # Check for empty or non-standard href values\n",
    "            if not href or href.strip() == '#' or href.strip().lower().startswith('javascript:'):\n",
    "                return 1  # Fake link detected\n",
    "\n",
    "            # You can add more criteria here to cover additional cases\n",
    "\n",
    "        return 0  # No fake link detected\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during the process\n",
    "        return 0  # Return 0 on error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def RightClickDisabled(url):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        return 0  # Unable to access the URL\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Initialize a flag to indicate whether right-click is disabled\n",
    "    right_click_disabled = False\n",
    "\n",
    "    # Check for scripts that may disable right-click\n",
    "    script_tags = soup.find_all('script')\n",
    "    for script in script_tags:\n",
    "        script_content = script.get_text()\n",
    "\n",
    "        # Check for various right-click disabling methods\n",
    "        if re.search(r'event\\.preventDefault\\(\\)', script_content):\n",
    "            right_click_disabled = True\n",
    "\n",
    "        if re.search(r'document\\.oncontextmenu', script_content):\n",
    "            right_click_disabled = True\n",
    "\n",
    "        if re.search(r'contextmenu|context-menu', script_content, re.I):\n",
    "            right_click_disabled = True\n",
    "\n",
    "        # You can add more checks here to cover additional sophisticated methods\n",
    "\n",
    "    # Check for attributes that may disable right-click\n",
    "    elements = soup.find_all(True)\n",
    "    for element in elements:\n",
    "        # Check for oncontextmenu attributes\n",
    "        if element.has_attr('oncontextmenu'):\n",
    "            right_click_disabled = True\n",
    "\n",
    "        # You can add more checks for other attributes or behaviors\n",
    "\n",
    "    if right_click_disabled:\n",
    "        return 1  # Right-click disabled detected\n",
    "    else:\n",
    "        return 0  # No right-click disabled detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def DetectPopUpWindow(url):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        return 0  # Unable to access the URL\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Search for elements that may trigger pop-up windows\n",
    "    pop_up_triggers = soup.find_all(['a', 'button', 'input', 'div'], href=True, onclick=True)\n",
    "\n",
    "    # Check if any elements have attributes indicating pop-up windows\n",
    "    if pop_up_triggers:\n",
    "        return 1  # Pop-up windows detected\n",
    "    else:\n",
    "        return 0  # No pop-up windows detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_email_submission(url):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        return 0  # Unable to access the URL\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Initialize a variable to track if any email submissions are found\n",
    "    found_email_submission = 0\n",
    "\n",
    "    # Search for email submission forms\n",
    "    forms = soup.find_all('form')\n",
    "    for form in forms:\n",
    "        if 'mailto:' in form.get('action', ''):\n",
    "            found_email_submission = 1  # Email submission form detected\n",
    "\n",
    "    # Search for email links using regular expressions\n",
    "    email_pattern = r\"[\\w\\.-]+@[\\w\\.-]+\"\n",
    "    text = soup.get_text()  # Extract text from the page\n",
    "    if re.search(email_pattern, text):\n",
    "        found_email_submission = 1  # Email address detected in text\n",
    "\n",
    "    return found_email_submission  # Return 1 if any email submissions are found, otherwise 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def IframeOrFrame(url):\n",
    "    \"\"\"\n",
    "    Checks if iframe or frame is present in the URL.\n",
    "    If iframe or frame is present then it is phishing hence the function returns 1,\n",
    "    and if iframe or frame is not present then it is benign hence return 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Check for iframes and frames in the HTML content\n",
    "    if soup.find_all('iframe') or soup.find_all('frame'):\n",
    "        return 1  # Phishing detected\n",
    "    else:\n",
    "        return 0  # No iframes or frames detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MissingTitle(url):\n",
    "    # Import the BeautifulSoup and requests libraries\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    # Get the HTML source code from the URL\n",
    "    html = requests.get(url).text\n",
    "    # Create a soup object from the HTML\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Find the title element in the HTML\n",
    "    title = soup.find(\"title\")\n",
    "    # Check if the title element exists or not\n",
    "    if title:\n",
    "        # Return 0 if the title element exists\n",
    "        return 0\n",
    "    else:\n",
    "        # Return 1 if the title element does not exist\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImagesOnlyInForm(url):\n",
    "  # Import the BeautifulSoup and requests libraries\n",
    "  from bs4 import BeautifulSoup\n",
    "  import requests\n",
    "  # Get the HTML source code from the URL\n",
    "  html = requests.get(url).text\n",
    "  # Create a soup object from the HTML\n",
    "  soup = BeautifulSoup(html, \"html.parser\")\n",
    "  # Find all the image tags in the HTML\n",
    "  images = soup.find_all(\"img\")\n",
    "  # Initialize a counter for images only in form tags\n",
    "  img_in_form = 0\n",
    "  # Loop through each image and check its parent tag\n",
    "  for image in images:\n",
    "    # Get the parent tag of the image\n",
    "    parent = image.parent.name\n",
    "    # Check if the parent tag is a form tag\n",
    "    if parent == \"form\":\n",
    "      # Increment the counter\n",
    "      img_in_form += 1\n",
    "  # Calculate the percentage of images only in form tags\n",
    "  pct = img_in_form / len(images) * 100\n",
    "  # Return 0 if the percentage is less than or equal to 50, and 1 if it's greater than 50\n",
    "  if pct <= 50:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SubdomainLevelRT(url):\n",
    "  # Import the urlparse library to parse the URL\n",
    "  from urllib.parse import urlparse\n",
    "  # Parse the URL and get the domain name\n",
    "  domain = urlparse(url).netloc\n",
    "  # Remove the port number or other suffixes from the domain name if any\n",
    "  domain = domain.split(\":\")[0]\n",
    "  # Split the domain name by dots and get the number of parts\n",
    "  parts = domain.split(\".\")\n",
    "  # The number of subdomains is the number of parts minus two (the top-level domain and the root domain)\n",
    "  subdomains = len(parts) - 2\n",
    "  # Calculate the percentage of subdomains\n",
    "  percentage = subdomains / len(parts)\n",
    "  # Return -1 if the percentage is less than 0.2, 0 if it's between 0.2 and 0.5, and 1 if it's greater than 0.5\n",
    "  if percentage < 0.2:\n",
    "    return -1\n",
    "  elif percentage <= 0.6:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PctExtResourceUrlsRT(url):\n",
    "  # Import the BeautifulSoup, requests, and urlparse libraries\n",
    "  from bs4 import BeautifulSoup\n",
    "  import requests\n",
    "  from urllib.parse import urlparse\n",
    "  # Get the HTML source code from the URL\n",
    "  html = requests.get(url).text\n",
    "  # Create a soup object from the HTML\n",
    "  soup = BeautifulSoup(html, \"html.parser\")\n",
    "  # Find all the resource tags in the HTML\n",
    "  resources = soup.find_all([\"img\", \"script\", \"link\", \"style\", \"font\"])\n",
    "  # Initialize a counter for resources with external URLs\n",
    "  ext_resources = 0\n",
    "  # Loop through each resource and check its attributes\n",
    "  for resource in resources:\n",
    "    # Check if the resource has an 'href' or 'src' attribute\n",
    "    if 'href' in resource.attrs or 'src' in resource.attrs:\n",
    "      # Get the URL from the attribute\n",
    "      res_url = resource.attrs.get('href') or resource.attrs.get('src')\n",
    "      # Parse the resource URL and get the domain name\n",
    "      res_domain = urlparse(res_url).netloc\n",
    "      # Parse the webpage URL and get the domain name\n",
    "      web_domain = urlparse(url).netloc\n",
    "      # Check if the domain names are different\n",
    "      if res_domain != web_domain:\n",
    "        # Increment the counter\n",
    "        ext_resources += 1\n",
    "  # Calculate the percentage of resources with external URLs\n",
    "  pct = ext_resources / len(resources) * 100\n",
    "  # Return the percentage as a categorical value based on some thresholds\n",
    "  if pct < 10:\n",
    "    return -1\n",
    "  elif pct < 60:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PctExtResourceUrlsRT(url):\n",
    "  import requests\n",
    "\n",
    "  html = requests.get(url).text\n",
    "  \n",
    "  # Import the BeautifulSoup library to parse the HTML\n",
    "  from bs4 import BeautifulSoup\n",
    "  # Create a soup object from the HTML\n",
    "  soup = BeautifulSoup(html, \"html.parser\")\n",
    "  # Find all the resource tags in the HTML\n",
    "  resources = soup.find_all([\"img\", \"script\", \"link\", \"style\", \"font\"])\n",
    "  # Initialize a counter for resources with external URLs\n",
    "  ext_resources = 0\n",
    "  # Loop through each resource and check its attributes\n",
    "  for resource in resources:\n",
    "    # Check if the resource has an 'href' or 'src' attribute\n",
    "    if 'href' in resource.attrs or 'src' in resource.attrs:\n",
    "      # Get the URL from the attribute\n",
    "      res_url = resource.attrs.get('href') or resource.attrs.get('src')\n",
    "      # Get the domain name of the resource URL\n",
    "      res_domain = res_url.split(\"//\")[-1].split(\"/\")[0]\n",
    "      # Get the domain name of the webpage URL\n",
    "      web_domain = url.split(\"//\")[-1].split(\"/\")[0]\n",
    "      # Check if the domain names are different\n",
    "      if res_domain != web_domain:\n",
    "        # Increment the counter\n",
    "        ext_resources += 1\n",
    "  # Calculate the percentage of resources with external URLs\n",
    "  pct = ext_resources / len(resources) * 100\n",
    "  # Return the percentage as a categorical value based on some thresholds\n",
    "  if pct < 10:\n",
    "    return -1\n",
    "  elif pct < 60:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AbnormalExtFormActionR(url):\n",
    "\n",
    "    import requests\n",
    "    html = requests.get(url).text\n",
    "    # Import the BeautifulSoup library to parse the HTML\n",
    "    from bs4 import BeautifulSoup\n",
    "    # Create a soup object from the HTML\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Find all the form tags in the HTML\n",
    "    forms = soup.find_all(\"form\")\n",
    "    # Initialize a counter for forms with external URLs\n",
    "    ext_forms = 0\n",
    "    # Loop through each form and check its action attribute\n",
    "    for form in forms:\n",
    "        # Get the action attribute of the form\n",
    "        action = form.get(\"action\")\n",
    "        # Check if the action is not None or empty\n",
    "        if action:\n",
    "            # Get the domain name of the action URL\n",
    "            domain = action.split(\"//\")[-1].split(\"/\")[0]\n",
    "            # Get the domain name of the HTML\n",
    "            html_domain = html.split(\"//\")[-1].split(\"/\")[0]\n",
    "            # Check if the domain names are different\n",
    "            if domain != html_domain:\n",
    "                # Increment the counter\n",
    "                ext_forms += 1\n",
    "    # Calculate the percentage of forms with external URLs\n",
    "    pct = ext_forms / len(forms) * 100\n",
    "    # Return the percentage as a categorical value based on some thresholds\n",
    "    if pct < 10:\n",
    "        return -1\n",
    "    elif pct < 60:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtMetaScriptLinkRT(url):\n",
    "    import requests\n",
    "\n",
    "    html = requests.get(url).text\n",
    "    # Import the BeautifulSoup library to parse the HTML\n",
    "    from bs4 import BeautifulSoup\n",
    "    # Create a soup object from the HTML\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Find all the meta, script, and link tags in the HTML\n",
    "    tags = soup.find_all([\"meta\", \"script\", \"link\"])\n",
    "    # Initialize a counter for tags with external URLs\n",
    "    ext_tags = 0\n",
    "    # Loop through each tag and check its attributes\n",
    "    for tag in tags:\n",
    "        # Check if the tag has an 'href' or 'src' attribute\n",
    "        if 'href' in tag.attrs or 'src' in tag.attrs:\n",
    "            # Get the URL from the attribute\n",
    "            url = tag.attrs.get('href') or tag.attrs.get('src')\n",
    "            # Get the domain name of the URL\n",
    "            domain = url.split(\"//\")[-1].split(\"/\")[0]\n",
    "            # Get the domain name of the HTML\n",
    "            html_domain = html.split(\"//\")[-1].split(\"/\")[0]\n",
    "            # Check if the domain names are different\n",
    "            if domain != html_domain:\n",
    "                # Increment the counter\n",
    "                ext_tags += 1\n",
    "    # Calculate the percentage of tags with external URLs\n",
    "    pct = ext_tags / len(tags) * 100\n",
    "    # Return the percentage as a categorical value based on some thresholds\n",
    "    if pct < 10:\n",
    "        return -1\n",
    "    elif pct < 60:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PctExtNullSelfRedirectHyperlinksRT(html):\n",
    "  # Import the BeautifulSoup library to parse the HTML\n",
    "  from bs4 import BeautifulSoup\n",
    "  # Create a soup object from the HTML\n",
    "  soup = BeautifulSoup(html, \"html.parser\")\n",
    "  \n",
    "  # Find all the hyperlinks in the HTML\n",
    "  links = soup.find_all(\"a\")\n",
    "  # Initialize a counter for external or null links\n",
    "  ext_null_links = 0\n",
    "  # Loop through each link and check its attributes\n",
    "  for link in links:\n",
    "    # Get the href attribute of the link\n",
    "    href = link.get(\"href\")\n",
    "    # Check if the href is None, starts with \"#\", or contains \"javascript:void(0)\"\n",
    "    if href is None or href.startswith(\"#\") or \"javascript:void(0)\" in href:\n",
    "      # Increment the counter\n",
    "      ext_null_links += 1\n",
    "    else:\n",
    "      # Get the domain name of the link\n",
    "      domain = href.split(\"//\")[-1].split(\"/\")[0]\n",
    "      # Get the domain name of the HTML\n",
    "      html_domain = html.split(\"//\")[-1].split(\"/\")[0]\n",
    "      # Check if the domain names are different\n",
    "      if domain != html_domain:\n",
    "        # Increment the counter\n",
    "        ext_null_links += 1\n",
    "  # Calculate the percentage of external or null links\n",
    "  pct = ext_null_links / len(links) * 100\n",
    "  # Return the percentage as a categorical value based on some thresholds\n",
    "  if pct < 10:\n",
    "    return -1\n",
    "  elif pct < 60:\n",
    "    return 0\n",
    "  else:\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(url):\n",
    "    \"\"\"\n",
    "    Extracts all the features from the URL and returns a list of the features.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    features.append(NumDots(url))\n",
    "    features.append(SubdomainLevel(url))\n",
    "    features.append(PathLevel(url))\n",
    "    features.append(UrlLength(url))\n",
    "    features.append(NumDashInHostName(url))\n",
    "    features.append(AtSymbol(url))\n",
    "    features.append(TildeSymbol(url))\n",
    "    features.append(NumUnderscore(url))\n",
    "    features.append(NumPercent(url))\n",
    "    features.append(NumQueryComponents(url))\n",
    "    features.append(NumAmpersand(url))\n",
    "    features.append(NumHash(url))\n",
    "    features.append(NumNumericChars(url))\n",
    "    features.append(NoHttps(url))\n",
    "    features.append(RandomString(url))\n",
    "    features.append(IpAddress(url))\n",
    "    features.append(DomainInSubdomains(url))\n",
    "    features.append(DomainInPaths(url))\n",
    "    features.append(HttpsInHostname(url))\n",
    "    features.append(HostnameLength(url))\n",
    "    features.append(PathLength(url))\n",
    "    features.append(QueryLength(url))\n",
    "    features.append(DoubleSlashInPath(url))\n",
    "    features.append(NumSensitiveWords(url))\n",
    "    features.append(EmbeddedBrandName(url))\n",
    "    features.append(PctExtHyperlinks(url))\n",
    "    features.append(PctExtResourceUrls(url))\n",
    "    features.append(ExtFavicon(url))\n",
    "    features.append(InsecureForms(url))\n",
    "    features.append(RelativeFormAction(url))\n",
    "    features.append(ExtFormAction(url))\n",
    "    features.append(AbnormalFormAction(url))\n",
    "    features.append(PctNullSelfRedirectHyperlinks(url))\n",
    "    features.append(FrequentDomainNameMismatch(url))\n",
    "    features.append(FakeLinkInStatusBar(url))\n",
    "    features.append(RightClickDisabled(url))\n",
    "    features.append(PopUpWindow(url))\n",
    "    features.append(SubmitInfoToEmail(url))\n",
    "    features.append(IframeOrFrame(url))\n",
    "    features.append(MissingTitle(url))\n",
    "    features.append(ImagesOnlyInForm(url))\n",
    "    features.append(SubdomainLevelRT(url))\n",
    "    features.append(UrlLengthRT(url))\n",
    "    features.append(PctExtResourceUrlsRT(url))\n",
    "    features.append(AbnormalExtFormActionR(url))\n",
    "    features.append(ExtMetaScriptLinkRT(url))\n",
    "    features.append(PctExtNullSelfRedirectHyperlinksRT(url))\n",
    "    \n",
    "    return features\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
